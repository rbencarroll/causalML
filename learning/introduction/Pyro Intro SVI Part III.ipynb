{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVI Part III ELBO Gradient Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We've defined a model and a guide. We'd like to maximize the log evidence $\\log p_{\\theta}(x)$ by maximizing the ELBO given by\n",
    "\n",
    "$ELBO≡Eqϕ(z)[logpθ(x,z)−logqϕ(z)]$\n",
    "\n",
    "To do this we're going to take gradient steps on the ELBO in the parameter space {$\\theta$, $\\phi$}, so we need to be able to compute unbiased estimates of \n",
    "\n",
    "$∇_{θ,ϕ}ELBO=∇_{θ,ϕ}E_{q_ϕ(z)}[logp_θ(x,z)−logq_ϕ(z)]$\n",
    "\n",
    "\n",
    "How do we do this for general stochastic functions? Generalize to, how do we compute gradients of expectations of an arbitrary cost function $f(z)$. Dropping any distinction between $\\theta$, and $\\phi$, We want to compute.\n",
    "\n",
    "$∇_ϕE_{q_ϕ(z)}[f_ϕ(z)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy Case Reparameterizeable Random Variables\n",
    "\n",
    "Suppose that we can reparameterize things such that\n",
    "\n",
    "$\\mathbb{E}_{q_{\\phi}({\\bf z})} \\left [f_{\\phi}({\\bf z}) \\right]\n",
    "=\\mathbb{E}_{q({\\bf \\epsilon})} \\left [f_{\\phi}(g_{\\phi}({\\bf \\epsilon})) \\right]$\n",
    "\n",
    "Critically, we've moved all the $\\phi$ dependence inside of the expectation. $q(\\epsilon)$ is a fixed distribution with no dependence on $\\phi$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tricky Case: Non-reparameterizable Random Variables\n",
    "\n",
    "* Many distributions cannot be reparameterized. For example all discrete distributions.\n",
    "\n",
    "*** calculus\n",
    "\n",
    "\n",
    "Reach REINFORCE Estimator, score function estimator, or likelihood ratio estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance\n",
    "\n",
    "Now have unbiased gradient estimator of expectations of cost functions. However, the score function estimator tends to have high variance, often too high to be usable. Two strategies to reduce variance below.\n",
    "\n",
    "1. Reduce variance by taking advantage of the particular structure of the cost function\n",
    "2. Reduce variance by using information from previous estimates (similar to momentum in stoch. grad. desc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causalML-xeA8SeKA",
   "language": "python",
   "name": "causalml-xea8seka"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
