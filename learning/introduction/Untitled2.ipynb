{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVI Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marking Conditional Independence in Pyro\n",
    "### Sequential Plate\n",
    "\n",
    "```python\n",
    "def model(data):\n",
    "    # sample f from the beta prior\n",
    "    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
    "    # loop over the observed data using pyro.sample with the obs keyword argument\n",
    "    for i in range(len(data)):\n",
    "        # observe datapoint i using the bernoulli likelihood\n",
    "        pyro.sample('obs_{}'.format(i), dist.Bernoulli(f), obs=data[i])\n",
    "```\n",
    "\n",
    "For this model, the observations are conditionally independent given the latent random variable `latent_fairness`. To explicitly mark thi sin Pyro we basically just need to replace the Python builtin `range` with the Pyro construct `plate`:\n",
    "\n",
    "```python\n",
    "def model(data):\n",
    "    f = pyro.sample('latent_fairness', dist.Beta(alpha0, beta0))\n",
    "    for i in pyro.plate('data_loop', len(data)):\n",
    "        pyro.sample('obs_{}'.format(i), dist.Bernoulli(f), obs=data[i])\n",
    "```\n",
    "\n",
    "plate is very similar to range, but each invocation of plate requires a unique name.\n",
    "\n",
    "In detail:\n",
    "* each observed `pyro.sample` statement occurs within a different execution of the body of the `for` loop, Pyro marks each observation as independent\n",
    "* this independence is properly a _conditional_ independence _given_ `latent_fairness` because `latent_fairness` is sampled outside the context of `data_loop`\n",
    "\n",
    "Gotchas\n",
    "\n",
    "THIS CODE IS WRONG\n",
    "```python\n",
    "my_reified_list = list(pyro.plate('data_loop', len(data)))\n",
    "for i in my_reified_list:\n",
    "    pyro.sample('obs_{}'.format(i), dist.Bernoulli(f), obs=data[i])\n",
    "```\n",
    "\n",
    "Pyro plate is not approprate for temporal models where each iteration of a loop depends on the previous iteration, `range` or `pyro.markov` should be used instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized Plate\n",
    "\n",
    "1. need data to be a tensor\n",
    "\n",
    "```python\n",
    "data = torch.zeros(10)\n",
    "data[0:6] = torch.ones(6) # 6 heads 4 tails\n",
    "```\n",
    "\n",
    "```python\n",
    "with pyro.plate('observe_data'):\n",
    "    pyro.sample('obs' dist.Bernoulli(f), obs=data)\n",
    "```\n",
    "\n",
    "* both require unique name\n",
    "* this code snippet only introduces a single observed random variable (obs) since the entire tensor is considered once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling\n",
    "\n",
    "#### Automatic Subsampling with `plate`\n",
    "\n",
    "Simplest case of subsampling.\n",
    "\n",
    "```python\n",
    "for i in pyro.plate('data_loop', len(data), subsample_size=5):\n",
    "    pyro.sample('obs_{}'.format(i), dist.Bernoulli(f), obs=data[i])\n",
    "```\n",
    "\n",
    "This will use 5 randomly chosen data points.\n",
    "\n",
    "\n",
    "With vectorized `plate`\n",
    "```python\n",
    "with plate('observe_data', size=10, subsample_size=5) as ind:\n",
    "    pyro.sample('obs', dist.Bernoulli(f),\n",
    "                obs=data.index_select(0, ind))\n",
    "```\n",
    "\n",
    "This causes plate to return a tensor of indices. User must pass a `device` argument to `plate` if `data` is on the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Subsampling strategies with `plate`\n",
    "\n",
    "Random selection means that some datapoints are likely never sampled. User can control this with the `subsample` argument to `plate`.\n",
    "\n",
    "### Subsampling when there are only local random variables\n",
    "\n",
    "only local random variables like vanilla VAE is a special case where `subsample_size` and `subsample` are not used.\n",
    "\n",
    "### Subsampling both global and local random variables\n",
    "\n",
    "Intentionally leaving out some text here...\n",
    "\n",
    "```python\n",
    "def model(data):\n",
    "    beta = pyro.sample('beta', ...)\n",
    "    for i in pyro.plate('locals', len(data):\n",
    "        z_i = pyro.sample('z_{i}'.format(i), ...)\n",
    "        # compute the parameter used to define the observation likelihood using the local random variable\n",
    "        theta_i = compute_something(z_i)\n",
    "        pyro.sample(\"obs_{}\".format(i), dist.MyDist(theta_i), obs=data[i])\n",
    "```\n",
    "* Note have `pyro.sample` statements both inside and outside the `plate` loop.\n",
    "\n",
    "```python\n",
    "def guide(data):\n",
    "    beta = pyro.sample('beta', ...) # sample the global RV\n",
    "    for i in pyro.plate('locals', len(data), subsample_size=5):\n",
    "        # sample the local RVs\n",
    "        pyro.sample('z_{}'.format(i), ..., lambda_i)\n",
    "```\n",
    "* Note that the indices will only be subsampled once in the guide. the pyro backend makes sure that the same set of indices are used during the execution of the model. For this reason, `subsample_size` only needs to be specified in the guide.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amortization\n",
    "\n",
    "Lets consider a bmodel with global and local latent random variables and local variational parameters...\n",
    "\n",
    "\n",
    "Instead of introducing local variational parameters, we're going to learn a single parametric function $f(\\cdot)$ and work with a variational distribution that has the form.\n",
    "\n",
    "q(beta) PI q(z_i | f(x_i))\n",
    "\n",
    "The function $f(\\cdot)$ which maps a given observation to a set of variational parameters tailored to that datapoint - will need to be sufficiently rich to capture the posterior accurately, but now we can handle large datasets without having to introduce an obscene number of variational parameters. This approach has other benefits too. For example, during learning $f(\\cdot)$ effectively allows us to share statistical power among different datapoints. Note that this is precistely the approach used in VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causalML-xeA8SeKA",
   "language": "python",
   "name": "causalml-xea8seka"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
